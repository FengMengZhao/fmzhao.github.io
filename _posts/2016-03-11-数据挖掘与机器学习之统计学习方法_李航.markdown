---
layout: post
title: 数据挖掘与机器学习之统计学习方法_李航
category: diary
---

### 统计学习方法概论

统计学习(statistical learning)是关于计算机基于数据构建概论统计模型并运用模型对数据进行预测和分析的一门科学。统计学习也称为统计机器学习(statistical maching learning)。

统计学习方法的三要素：模型(model)、策略(strategy)和算法(algorithm)

统计学习方法的步骤：

(1) 得到一个有限的数据训练集合

(2) 确定包含所有可能模型的假设空间，即学习模型的集合

(3) 确定模型选择的准则，即学习的策略

(4) 实现求解最优模型的算法，即学习算法

(5) 通过学习方法选择最优模型

(6) 利用学习从最优模型对新数据进行预测或者分析

监督学习利用训练数据集学习一个模型，再用模型对测试样本集进行预测(prediction)。由于在这个过程中需要训练数据集，而训练数据集往往是人工给出的，所以称为监督学习。监督学习分为学习和预测两个过程，由学习系统和预测系统完成。

过拟合现象：一味追求提高对训练数据的预测能力，所选模型的复杂度往往会比真是模型的更高，这是由于学习时所选择的模型包含的参数过多，以至于出现这一模型对一致数据预测的很好，但对未知数据预测很差的情况。也就是说，模型选择的时候，不仅仅要考虑对已知数据的预测能力，而且还要考虑对未知数据的预测能力

模型选择的典型方法是正则化(regularization)，正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或罚项(penalty term)

另一种常用的模型选择方法是交叉验证(cross validation)，交叉验证的思想是重复使用数据，把给定的数据进行切分，将切分的数据集组合为训练集与测试集，在此基础上反复进行训练、测试以及模型的选择

> 在给定样本数据充足的情况下，进行模型选择的简单方法随机的将数据集切分为三个部分，分别为训练集(training set)、验证集(validation set)和测试集(test set)。训练集用来训练模型，验证机用来进行模型的选择，而测试集用来最终对学习方法的评估。在学习到的不同复杂度的模型当中，选择对验证集有最小误差的模型。由于验证集有足够多的数据，所以用它对模型选择也是有效的。

> 而使用用交叉验证是在数据集是不充足的情况下进行的

泛化能力(generalization ability)：指有该方法学习到的模型对未知数据的预测能力，是学习方法本质上最重要的性质。

#### 分类问题

分类是监督学习的一个核心问题。监督学习从数据中学习一个分类模型或者或者分类决策函数，称为分类器(classifier)。分类器对新的输入进行输出预测，称之为分类。

> 分类的一个简单的例子是文本分类。文本可以是新闻报道、网页、电子邮件、学术论文等。类别往往是关于文本内容的，例如：政治、经济、体育等；也有关于文本特点的，例如：正面意见、反面意见；还可以根据应用确定，例如：垃圾邮件和非垃圾邮件等

#### 标注问题

标注(tagging)也是一个监督学习问题。可以认为标注问题是分类问题的一个推广，标注问题又是更复杂的结构预测(structure prediction)问题的简单形式。标注问题的输入是一个观测序列，输入是一个标记序列或者状态序列。标注问题的目标在于学习一个模型，使它能够对观测序列给出标记序列作为预测。

#### 回归问题

回归是监督学习的另一个重要问题。回归用于预测输入变量(自变量)和输出变量(因变量)只见的关系，特别是输入变量的值发生变化的时候，输出变量的值随之发生变化。回归模型是表示从输入变量到输出变量之间映射的函数。回归问题的学习等价与函数拟合：选择一条函数曲线使其很好的拟合已知数据且很好的预测位置数据。

回归问题按照输入变量的个数，一元回归和多元回归；按照输入变量和输出变量之间的关系的类别，分为线性回归和非线性回归。

> 回归问题最常用的损失函数是平方损失函数，在此情况下，回归问题可以由著名的最小二乘法(least squares)求解

***

***

### 感知机

感知机是二类分类的线性分类模型，其输入为实例特征向量，输入为实例的类别，取+1和-1二值。

感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面，为此导入基于误分类的损失函数，利用梯度下降发对损失函数进行极小化，取得感知机模型。

***

***

### k近邻法

k近邻法(k-nearest neighbor, k-NN)是一种基本分类与回归方法。

k近邻法的输入为实例的特征向量，对应特征空间的点，输出为实例的类别，可以取多类。

k近邻法假设给定一个训练数据集，其中的实例类别已定。分类时对新的实例，根据其k个近邻的训练实例的类别，通过多少多数表决等方式进行预测。

#### k近邻法的实现——kd树

***

***

### 朴素贝叶斯法

朴素贝叶斯(naive Bayes)法是基于贝叶斯定理与特征条件独立假设的分类方法。

对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入x，利用贝叶斯定理求出后验概率输出的最大概率y。

#### 朴素贝叶斯算法(naive Bayes algorithm)

***

***

### 决策树

决策树(decision tree)是一种基本的分类和回归方法。

决策树模型呈现树状结构，在分类问题中，表示基于特征对实例进行分类的过程。学习时，利用训练数据，根据损失函数最小化的原则建立决策树模型。与测试，对新的数据，利用决策树模型进行分类。

决策树学习通常包括三个步骤：特征选择、决策树的生成和决策树的修剪。

#### 决策树模型

分类决策树模型是一种描述对示例进行分类的树形结构。决策树由结点(Node)和有向边(directed edge)组成。结点有两种类型：内部结点(internal node)和页结点(leaf node)。内部结点表示一个特征或者属性，叶结点表示一个类。

用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试的结果，将实例分配到其子节点。这时，每一个子节点对应着该特征的一个取值。如此递归的对实例进行测试并分配，直到达到叶结点。最后将实例分到叶结点的类中。

#### ID3算法

#### C4.5生成算法

#### CART(classfication and regression tree, CART)算法

***

***

### 逻辑斯蒂回归于最大熵模型

***

***

### 支持向量机

支持向量机(support vector machines, SVM)是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。

***

***

### 提升方法

提升(boosting)方法是一种常用的统计学习方法，应用广泛且有效。在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提过分类的性能。

#### 提升方法AdaBoost算法

***

***

### EM算法及其推广

***

***

### 隐马尔科发模型(hidden Markov model, HMM)

***

***

### 条件随机场(conditional random field, CRF)

***

***